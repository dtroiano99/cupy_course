{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "0\n",
      "0\n",
      "0\n",
      "400\n",
      "512\n",
      "512\n",
      "0\n",
      "0\n",
      "512\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import cupy\n",
    "import numpy\n",
    "\n",
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()\n",
    "\n",
    "# Create an array on CPU.\n",
    "# NumPy allocates 400 bytes in CPU (not managed by CuPy memory pool).\n",
    "a_cpu = numpy.ndarray(100, dtype=numpy.float32)\n",
    "print(a_cpu.nbytes)                      # 400\n",
    "\n",
    "# You can access statistics of these memory pools.\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 0\n",
    "print(pinned_mempool.n_free_blocks())    # 0\n",
    "\n",
    "# Transfer the array from CPU to GPU.\n",
    "# This allocates 400 bytes from the device memory pool, and another 400\n",
    "# bytes from the pinned memory pool.  The allocated pinned memory will be\n",
    "# released just after the transfer is complete.  Note that the actual\n",
    "# allocation size may be rounded to larger value than the requested size\n",
    "# for performance.\n",
    "a = cupy.array(a_cpu)\n",
    "print(a.nbytes)                          # 400\n",
    "print(mempool.used_bytes())              # 512\n",
    "print(mempool.total_bytes())             # 512\n",
    "print(pinned_mempool.n_free_blocks())    # 1\n",
    "\n",
    "# When the array goes out of scope, the allocated device memory is released\n",
    "# and kept in the pool for future reuse.\n",
    "a = None  # (or `del a`)\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 512\n",
    "print(pinned_mempool.n_free_blocks())    # 1\n",
    "\n",
    "# You can clear the memory pool by calling `free_all_blocks`.\n",
    "mempool.free_all_blocks()\n",
    "pinned_mempool.free_all_blocks()\n",
    "print(mempool.used_bytes())              # 0\n",
    "print(mempool.total_bytes())             # 0\n",
    "print(pinned_mempool.n_free_blocks())    # 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "from cupyx import jit\n",
    "\n",
    "@jit.rawkernel()\n",
    "def elementwise_copy(x, y, size):\n",
    "    tid = jit.blockIdx.x * jit.blockDim.x + jit.threadIdx.x\n",
    "    ntid = jit.gridDim.x * jit.blockDim.x\n",
    "    for i in range(tid, size, ntid):\n",
    "        y[i] = x[i]\n",
    "\n",
    "size = cupy.uint32(2 ** 22)\n",
    "x = cupy.random.normal(size=(size,), dtype=cupy.float32)\n",
    "y = cupy.empty((size,), dtype=cupy.float32)\n",
    "\n",
    "elementwise_copy((128,), (1024,), (x, y, size))  # RawKernel style\n",
    "assert (x == y).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_gpu = cp.cuda.Event()\n",
    "end_gpu = cp.cuda.Event()\n",
    "\n",
    "start_gpu.record()\n",
    "start_cpu = time.perf_counter()\n",
    "out = my_func(a)\n",
    "end_cpu = time.perf_counter()\n",
    "end_gpu.record()\n",
    "end_gpu.synchronize()\n",
    "t_gpu = cp.cuda.get_elapsed_time(start_gpu, end_gpu)\n",
    "t_cpu = end_cpu - start_cpu"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cupy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
